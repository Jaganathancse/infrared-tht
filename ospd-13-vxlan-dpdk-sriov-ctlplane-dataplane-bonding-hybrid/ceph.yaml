parameter_defaults:
  CephPoolDefaultSize: 1
  CephPoolDefaultPgNum: 32
  CephPools:
    - {"name": backups, "pg_num": 32, "pgp_num": 32, "application": "rbd"}
    - {"name": volumes, "pg_num": 512, "pgp_num": 512, "application": "rbd"}
    - {"name": vms, "pg_num": 128, "pgp_num": 128, "application": "rbd"}
    - {"name": images, "pg_num": 64, "pgp_num": 64, "application": "rbd"}
  CephConfigOverrides:
    osd_recovery_op_priority: 3
    osd_recovery_max_active: 3
    osd_max_backfills: 1
  CephAnsibleExtraConfig:
    nb_retry_wait_osd_up: 60
    delay_wait_osd_up: 20
    is_hci: true
    # 4 OSDs * 1 vCPUs per HDD = 4 vCPUs below not used by VNF on numa node0
    # fultonj: 6 OSDs so could increase to 6 vCPUs iff you have them, else use 3
    ceph_osd_docker_cpuset_cpus: "5,25,7,27,9,29"
    # cpu_limit 0 means no limit as we are limiting CPUs with cpuset above
    ceph_osd_docker_cpu_limit: 0
    # numactl preferred to cross the numa boundary if we have to but try to only use memory from numa node0
    # cpuset-mems would not let it cross numa boundary (boundary crossing is preferable to crashing)
    # lots of memory so NUMA boundary crossing unlikely
    ceph_osd_numactl_opts: "-N 1 --preferred=1"
  CephAnsibleDisksConfig:
    osds_per_device: 2
    osd_scenario: lvm
    osd_objectstore: bluestore
    devices:
      - /dev/sda
      - /dev/sdb
      - /dev/sdc
